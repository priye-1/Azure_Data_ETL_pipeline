{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c89ccf80-39fa-4e08-ac3c-cedc96cbcf85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# function to write data from all processed files to Azure SQL\n",
    "\n",
    "def write_to_azure_sql(df, table, mode):\n",
    "    username=\"database_username\"\n",
    "    password = dbutils.secrets.get(scope = \"scope_name\", key = \"scope_password\")\n",
    "    jdbcHostname = \"database_server\"\n",
    "    jdbcPort = 1433\n",
    "    jdbcDatabase = \"database_name\"\n",
    "    jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "    properties = {\n",
    "        \"user\" : username,\n",
    "        \"password\" : password,\n",
    "        \"driver\" : jdbcDriver\n",
    "    }\n",
    "\n",
    "    url = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname,jdbcPort,jdbcDatabase)\n",
    "\n",
    "    df.write.jdbc(url=url, table=table, mode=mode, properties = properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd042700-bc70-4a7c-91c4-da1d20114a8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading csv files, casting strings to integers and writing to azure SQL\n",
    "\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".options(header='true', delimeter=',')\\\n",
    ".load(\"/mnt/landing/SourceA*.csv\")\n",
    "\n",
    "# check schema\n",
    "df.printSchema()\n",
    "\n",
    "df_csv = df.select(\n",
    "    \"Employee_id\",\n",
    "    \"First_Name\",\n",
    "    \"Last_Name\",\n",
    "    \"Gender\",\n",
    "    \"Salary\",\n",
    "    \"Date_of_Birth\",\n",
    "    \"Age\",\n",
    "    \"Country\",\n",
    "    \"Department_id\",\n",
    "    \"Date_of_Joining\",\n",
    "    \"Manager_id\",\n",
    "    \"Currency\",\n",
    "    \"End_Date\"\n",
    ")\n",
    "# create a temporary view \"emp\"\n",
    "df_csv.createOrReplaceTempView(\"Emp\")\n",
    "\n",
    "# casting certain columns\n",
    "df_cast = spark.sql(\n",
    "    \"select cast(Employee_id as int), cast(Age as int), First_Name, Last_Name, Gender, cast(Salary as int), Date_Of_Birth, Country, cast(Department_id as int), Date_of_Joining, cast(Manager_id as int), Currency, End_Date from Emp\"\n",
    ")\n",
    "\n",
    "#alternative form of casting\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "ds = df_csv.withColumn(\n",
    "    \"Employee_id\",\n",
    "    df_csv.Employee_id.cast(IntegerType())\n",
    ").withColumn(\n",
    "    \"Salary\",\n",
    "    df_csv.Salary.cast(IntegerType())\n",
    ")\n",
    "\n",
    "# confirm data casting\n",
    "# print(ds.printSchema())\n",
    "# display(df_cast)\n",
    "\n",
    "# write data to delta table\n",
    "df_cast.write.mode(\"overwrite\")\\\n",
    ".format(\"delta\")\\\n",
    ".saveAsTable(\"dvdb.employee\")\n",
    "\n",
    "\n",
    "# write data to Azure SQL\n",
    "write_to_azure_sql(df_cast, table=\"Employee\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fe09c53-e1a9-4335-8123-9e0542a3e3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0df8748-dc42-473f-9f56-b3da582151cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading json file, casting values to integers and writing to azure SQL\n",
    "# read json file\n",
    "df = spark.read.format(\"json\")\\\n",
    ".option('header', 'true')\\\n",
    ".load(\"/mnt/landing/SourceB*.json\")\n",
    "\n",
    "# check schema\n",
    "df.printSchema()\n",
    "\n",
    "df_json = df.select(\n",
    "    \"Employee_id\",\n",
    "    \"Customer.First_Name\",\n",
    "    \"Customer.Last_Name\",\n",
    "    \"Gender\",\n",
    "    \"Salary\",\n",
    "    \"Date_of_Birth\",\n",
    "    \"Age\",\n",
    "    \"Country\",\n",
    "    \"Department_id\",\n",
    "    \"Date_of_Joining\",\n",
    "    \"Manager_id\",\n",
    "    \"Currency\",\n",
    "    \"End_Date\"\n",
    ")\n",
    "\n",
    "# create a temporary view \"EmpJson\"\n",
    "df_json.createOrReplaceTempView(\"EmpJson\")\n",
    "\n",
    "# casting certain columns\n",
    "df_json_cast = spark.sql(\n",
    "    \"select cast(Employee_id as int), cast(Age as int), First_Name, Last_Name, Gender, cast(Salary as int), Date_Of_Birth, Country, cast(Department_id as int), Date_of_Joining, cast(Manager_id as int), Currency, End_Date from EmpJson\"\n",
    ")\n",
    "\n",
    "# confirm data casting\n",
    "# display(df_cast)\n",
    "\n",
    "df_json_cast.write.mode(\"append\")\\\n",
    ".format(\"delta\")\\\n",
    ".saveAsTable(\"dvdb.employee\")\n",
    "\n",
    "# write data to Azure SQL\n",
    "write_to_azure_sql(df_json_cast, table=\"Employee\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "494260ad-6e1a-4776-bcb9-46a18d67928b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ee25e2-d1c6-4dc7-a5cd-d4aee712c64d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading xml file, casting strings to integers and writing to azure SQL\n",
    "\n",
    "df = spark.read.format(\"com.databricks.spark.xml\")\\\n",
    ".option(\"rootTag\", \"dataset\")\\\n",
    ".option(\"rowTag\", \"record\")\\\n",
    ".load(\"/mnt/landing/SourceC*.xml\")\n",
    "\n",
    "# read json file\n",
    "df_xml = df.select(\n",
    "    \"Employee_id\",\n",
    "    \"Customer.First_Name\",\n",
    "    \"Customer.Last_Name\",\n",
    "    \"Gender\",\n",
    "    \"Salary\",\n",
    "    \"Date_of_Birth\",\n",
    "    \"Age\",\n",
    "    \"Country\",\n",
    "    \"Department_id\",\n",
    "    \"Date_of_Joining\",\n",
    "    \"Manager_id\",\n",
    "    \"Currency\",\n",
    "    \"End_Date\"\n",
    ")\n",
    "df_xml.show()\n",
    "\n",
    "# create a temporary view \"emp\"\n",
    "df_xml.createOrReplaceTempView(\"Emp\")\n",
    "\n",
    "# casting certain columns\n",
    "df_xml_cast = spark.sql(\n",
    "    \"select cast(Employee_id as int), cast(Age as int), First_Name, Last_Name, Gender, cast(Salary as int), Date_Of_Birth, Country, cast(Department_id as int), Date_of_Joining, cast(Manager_id as int), Currency, End_Date from Emp\"\n",
    ")\n",
    "\n",
    "df_xml_cast.write.mode(\"append\")\\\n",
    ".format(\"delta\")\\\n",
    ".saveAsTable(\"dvdb.employee\")\n",
    "\n",
    "write_to_azure_sql(df_xml_cast, table=\"Employee\", mode=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9a3b5e8-957d-4b1d-b5fb-7fed4005ba03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- confirm total number of rows in delta table\n",
    "\n",
    "select count(*) from dvdb.employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing Department csv file for visualization\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "custom_schema = StructType(\n",
    "    [\n",
    "        StructField(\"Department_id\", IntegerType(),True),\n",
    "        StructField(\"Name\", StringType(),True),\n",
    "    ]\n",
    ")  \n",
    "df = spark.read.format(\"csv\")\\\n",
    ".options(header='true', delimeter=',')\\\n",
    ".schema(custom_schema)\\\n",
    ".load(\"/mnt/landing/Department.csv\")\n",
    "\n",
    "\n",
    "# write dataframe to delta table\n",
    "df.write.mode(\"append\")\\\n",
    ".format(\"delta\")\\\n",
    ".saveAsTable(\"dvdb.department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-04-29 23:26:18",
   "notebookOrigID": 323630220315204,
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
